# -*- coding: utf-8 -*-
"""Neural net video face censor

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rwZBDczwmt1_R3l_YWJlVWIcnJs_EGhg

## Installation
NOTE: a restart needs to be performed after installing detectron 2
"""

!pip install -q cython pyyaml==5.1

!pip3 install "git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI"

!git clone https://github.com/albumentations-team/albumentations.git albu
!pip install -e albu

#!pip install -U albumentations

import albu

!git clone https://github.com/facebookresearch/detectron2 detectron2_repo
!git checkout e883afb
!pip install -e detectron2_repo

from google.colab import output
output.eval_js('new Audio("https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg").play()')

!pip install -q -U watermark

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext watermark
# %watermark -v -p numpy,pandas,pycocotools,torch,torchvision,detectron2

"""## Torch and detectron setup"""

# Commented out IPython magic to ensure Python compatibility.
import torch, torchvision
import detectron2 
from detectron2.utils.logger import setup_logger
setup_logger()

import glob

import os
import ntpath
import numpy as np
import cv2
import random
import itertools
import pandas as pd
from tqdm import tqdm
import urllib
import json
from PIL import ImageFilter

import PIL.Image as Image

from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor, DefaultTrainer
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer, ColorMode
from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.structures import BoxMode

import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)

HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]

sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))

rcParams['figure.figsize'] = 12, 8

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

# Download labeled dataset of faces from dataturks, hosted by amazon web services. The json itself is on a google drive with a public ID
# This json contains 409 labeled images
!gdown --id 1K79wJgmPTWamqb04Op2GxW0SW9oxw8KS

# Load json using pandas dataframe
faces_df = pd.read_json('face_detection.json', lines=True)
# Check that json loaded correctly into pandas
faces_df.head()

os.makedirs("faces", exist_ok=True)

dataset = []

# tqdm tracks and outputs the progress of a loop.
for index, row in tqdm(faces_df.iterrows(), total=faces_df.shape[0]):
    # Download image and only use RGB values.
    img = urllib.request.urlopen(row["content"])
    img = Image.open(img)
    img = img.convert('RGB')

    image_name = f'face_{index}.jpeg'

    img.save(f'faces/{image_name}', "JPEG")
    
    annotations = row['annotation']
    for an in annotations:

      data = {}

      width = an['imageWidth']
      height = an['imageHeight']
      points = an['points']

      data['file_name'] = image_name
      data['width'] = width
      data['height'] = height

      # points are stored as fractions of the image dimentions.
      # So we need to multiply the points with the correct dimention and round to the nearest whole number.
      data["x_min"] = int(round(points[0]["x"] * width))
      data["y_min"] = int(round(points[0]["y"] * height))
      data["x_max"] = int(round(points[1]["x"] * width))
      data["y_max"] = int(round(points[1]["y"] * height))

      data['class_name'] = 'face'

      dataset.append(data)

# Convert our dataset list into a pandas dataframe
df = pd.DataFrame(dataset)
df.head()

print(df.file_name.unique().shape[0], df.shape[0])

df.to_csv('annotations.csv', header=True, index=None)

# Test for att det virker
def annotate_image(annotations, resize=True):
  file_name = annotations.file_name.to_numpy()[0]
  img = cv2.cvtColor(cv2.imread(f'faces/{file_name}'), cv2.COLOR_BGR2RGB)

  for i, a in annotations.iterrows():    
    cv2.rectangle(img, (a.x_min, a.y_min), (a.x_max, a.y_max), (255, 255, 0), 2)

  if not resize:
    return img

  return cv2.resize(img, (384, 384), interpolation = cv2.INTER_AREA)

img_df = df[df.file_name == df.file_name.unique()[0]]
img = annotate_image(img_df, resize=False)

plt.imshow(img)
plt.axis('off');

img_df = df[df.file_name == df.file_name.unique()[1]]
img = annotate_image(img_df, resize=False)

plt.imshow(img)
plt.axis('off');

sample_images = [annotate_image(df[df.file_name == f]) for f in df.file_name.unique()[:10]]
sample_images = torch.as_tensor(sample_images)

sample_images.shape

sample_images = sample_images.permute(0, 3, 1, 2)

sample_images.shape

plt.figure(figsize=(24, 12))
grid_img = torchvision.utils.make_grid(sample_images, nrow=5)

plt.imshow(grid_img.permute(1, 2, 0))
plt.axis('off');

df = pd.read_csv('annotations.csv')

IMAGES_PATH = f'faces'

unique_files = df.file_name.unique()

train_files = set(np.random.choice(unique_files, int(len(unique_files) * 0.95), replace=False))
train_df = df[df.file_name.isin(train_files)]
test_df = df[~df.file_name.isin(train_files)]

train_df.head()

classes = df.class_name.unique().tolist()

# Konverter til Detectron2 bruk
def create_dataset_dicts(df, classes):
  dataset_dicts = []
  for image_id, img_name in enumerate(df.file_name.unique()):

    record = {}

    image_df = df[df.file_name == img_name]

    file_path = f'{IMAGES_PATH}/{img_name}'
    record["file_name"] = file_path
    record["image_id"] = image_id
    record["height"] = int(image_df.iloc[0].height)
    record["width"] = int(image_df.iloc[0].width)

    objs = []
    for _, row in image_df.iterrows():

      xmin = int(row.x_min)
      ymin = int(row.y_min)
      xmax = int(row.x_max)
      ymax = int(row.y_max)

      poly = [
          (xmin, ymin), (xmax, ymin), 
          (xmax, ymax), (xmin, ymax)
      ]
      poly = list(itertools.chain.from_iterable(poly))

      obj = {
        "bbox": [xmin, ymin, xmax, ymax],
        "bbox_mode": BoxMode.XYXY_ABS,
        "segmentation": [poly],
        "category_id": classes.index(row.class_name),
        "iscrowd": 0
      }
      objs.append(obj)

    record["annotations"] = objs
    dataset_dicts.append(record)
  return dataset_dicts

for d in ["train", "val"]:
  DatasetCatalog.register("faces_" + d, lambda d=d: create_dataset_dicts(train_df if d == "train" else test_df, classes))
  MetadataCatalog.get("faces_" + d).set(thing_classes=classes)

statement_metadata = MetadataCatalog.get("faces_train")

class CocoTrainer(DefaultTrainer):
  
  @classmethod
  def build_evaluator(cls, cfg, dataset_name, output_folder=None):

    if output_folder is None:
        os.makedirs("coco_eval", exist_ok=True)
        output_folder = "coco_eval"

    return COCOEvaluator(dataset_name, cfg, False, output_folder)

cfg = get_cfg()

cfg.merge_from_file(
  model_zoo.get_config_file(
    "COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml"
  )
)

cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(
  "COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml"
)

cfg.DATASETS.TRAIN = ("faces_train",)
cfg.DATASETS.TEST = ("faces_val",)
cfg.DATALOADER.NUM_WORKERS = 4

cfg.SOLVER.IMS_PER_BATCH = 4
cfg.SOLVER.BASE_LR = 0.001
cfg.SOLVER.WARMUP_ITERS = 1000
cfg.SOLVER.MAX_ITER = 1500
cfg.SOLVER.STEPS = (1000, 1500)
cfg.SOLVER.GAMMA = 0.05

cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64
cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes)

cfg.TEST.EVAL_PERIOD = 500

from google.colab import output
output.eval_js('new Audio("https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg").play()')

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)

trainer = CocoTrainer(cfg) 
trainer.resume_or_load(resume=False)
trainer.train() #denne må kjøre litt, men trenger ikke hele 2 timer

"""# PRELOADED MODEL

"""

#Fech pre-trained weights
!gdown --id 18Ev2bpdKsBaDufhVKf0cT6RmM3FjW3nL

!mv face_detector.pth output/model_final.pth

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.70

predictor = DefaultPredictor(cfg)

evaluator = COCOEvaluator("faces_val", cfg, False, output_dir="./output/")

val_loader = build_detection_test_loader(cfg, "faces_val")

inference_on_dataset(trainer.model, val_loader, evaluator)

os.makedirs("annotated_results", exist_ok=True)

test_image_paths = test_df.file_name.unique()

def __str__(self) -> str:
        s = self.__class__.__name__ + "("
        s += "num_instances={}, ".format(len(self))
        s += "image_height={}, ".format(self._image_size[0])
        s += "image_width={}, ".format(self._image_size[1])
        s += "fields=[{}])".format(", ".join((f"{k}: {v}" for k, v in self._fields.items())))
        return s

from google.colab.patches import cv2_imshow

from scipy import misc

import skimage
from skimage.viewer import ImageViewer

from PIL import ImageDraw, ImageFilter

os.makedirs("cropped",exist_ok=True)

"""### **custom**

"""

vid = os.makedirs("vid/",exist_ok=True)
saved = os.makedirs("saved/",exist_ok=True)
namevid = 0

ADD BILDER TIL VID FOLDER

import cv2
vidcap = cv2.VideoCapture('video.mp4')
success,image = vidcap.read()
count = 0
while success:
  cv2.imwrite("vid/frame%d.png" % count, image)     # save frame as JPEG file      
  success,image = vidcap.read()
  print('Read a new frame: ', success)
  count += 1

number = 0 
CUSTOM_PATHS = f'vid'
for custom in sorted(os.listdir("vid")):
  
  class Instances:
    def __init__(self, num_instances, image_height,image_width):
      self.num_instances=num_instances
      self.image_height=image_height
      self.image_width=image_width

  custom_name = ntpath.basename(custom)
  
  custom_path = f'{CUSTOM_PATHS}/{custom}'
  rawimage = Image.open(f"vid/{custom_name}")
  
  #croppename = "test.jpeg"
  #croppename = file_name.replace('.jpg','.png')
  im = cv2.imread(custom_path)
  
  colorBlack = [(0.3,0.3,0.5)]
  outputs = predictor(im)
  
  crods=[]
  
  #instances': Instances(num_instances=8
  
  numberofBoxes = len(outputs['instances'])
  value = outputs['instances']

  print("picture")
  if(numberofBoxes and im.any):
    for x in range(numberofBoxes):
     #print(value.scores.cpu().numpy()[x])
     #print(value.pred_boxes[x])
     x1=value.pred_boxes[x].tensor.cpu().numpy()[0][0]
     y1=value.pred_boxes[x].tensor.cpu().numpy()[0][1]
     x2=value.pred_boxes[x].tensor.cpu().numpy()[0][2]
     y2=value.pred_boxes[x].tensor.cpu().numpy()[0][3]
     #roi = im[int(y1):int(y2), int(x1):int(x2)]
     x1= int(x1) 
     x2= int(x2) 
     y1= int(y1) 
     y2= int(y2) 
     crods.append([x1,y1,x2,y2])

     #print(x1,x2,y1,y2)
     #if(x2<x1): x1, x2 = x2,x1
     #if(y2<y1): y1, y2 = y2,y1
     print(x)
     print(numberofBoxes)
     print(x1,x2,y1,y2)
     cropped = im[y1:y2,x1:x2,::-1]
     #cropped = im[x1:y1,x2:y2]
     plt.axis('off');
     #try:
     sig = im.size/20000
     if sig > 500 and sig < 1000:
       sig = 30
     elif sig > 1000:
       sig = 40
     else:
       sig = 15

     print("size",sig)
     
     
     blurred = skimage.filters.gaussian(cropped,sigma=15)
     
     #blurred = cropped.resize((16,16),resample=Image.BILINEAR)
     #imgSmall = cropped.resize((16,16),resample=Image.BILINEAR)

# Scale back up using NEAREST to original size
    # blurred = imgSmall.resize(img.size,Image.NEAREST)

     plt.imshow(blurred)
     
     plt.savefig(f'cropped/{custom_name}-{x}.jpeg', bbox_inches = 'tight',pad_inches = 0)
     #except ValueError:  #raised if `y` is empty.
       # pass
     #rawimage.paste(plt.imshow(blurred),(0,0),plt.imshow(blurred))
     #plt.show()
     #im1 = Image.open('testplot.png')
     #rawimage.paste(im1,(x1,y1))
     #os.remove("testplot.png")
     #rawimage.save(f'test/{file_path}', quality=100)

  #value = (value['instances'])


  v = Visualizer(

    im[:, :, ::-1],

    metadata=statement_metadata,

    scale=1.,

    instance_mode=ColorMode.SEGMENTATION

  )


  instances = outputs["instances"].to("cpu")

  instances.remove('pred_masks')

  v = v.draw_instance_predictions(instances)
  plt.axis('off')
  result = v.get_image()[:, :, ::-1]
  i = 0 
  for items in crods:
    
    crim = Image.open(f'cropped/{custom_name}-{i}.jpeg')
    crim=crim.resize((items[2]-items[0],items[3]-items[1]))
    i +=1
    rawimage.paste(crim,(items[0],items[1]))
    imgplot = plt.imshow(rawimage)
    plt.axis('off')
    plt.gcf()
    #plt.savefig("fuck.png")
    plt.savefig(f"saved/vid{number}", bbox_inches='tight')

    plt.show()
    
  number +=1
  #plt save to file
  write_res = cv2.imwrite(f'annotated_results/{custom_name}', result)

import numpy as np
import glob

img_array = []
for filename in sorted(glob.glob('/content/saved/*.png')):
    img = cv2.imread(filename)
    height, width, layers = img.shape
    size = (width,height)
    img_array.append(img)


out = cv2.VideoWriter('improved.avi',cv2.VideoWriter_fourcc(*'DIVX'), 7, size)
 
for i in range(len(img_array)):
    out.write(img_array[i])
out.release()

!zip -r /content/file.zip /content/saved/
from google.colab import files
files.download("/content/file.zip")

import math

!rm -rf saved
!rm -rf vid

"""LAG FUNKSJON FOR STYLEGAN SOM TAR IN BILDE HER OG RETURNERER ET BILDE """

for clothing_image in test_image_paths:
  
  class Instances:
    def __init__(self, num_instances, image_height,image_width):
      self.num_instances=num_instances
      self.image_height=image_height
      self.image_width=image_width

  file_name = ntpath.basename(clothing_image)
  file_path = f'{IMAGES_PATH}/{clothing_image}'
  rawimage = Image.open(file_path)
  croppename = file_name.replace('.jpeg','')
  im = cv2.imread(file_path)
  
  colorBlack = [(0.3,0.3,0.5)]
  outputs = predictor(im)
  
  crods=[]

  #instances': Instances(num_instances=8
  
  numberofBoxes = len(outputs['instances'])
  value = outputs['instances']

  print("picture")
  if(numberofBoxes and im.any):
    for x in range(numberofBoxes):
     x1=value.pred_boxes[x].tensor.cpu().numpy()[0][0]
     y1=value.pred_boxes[x].tensor.cpu().numpy()[0][1]
     x2=value.pred_boxes[x].tensor.cpu().numpy()[0][2]
     y2=value.pred_boxes[x].tensor.cpu().numpy()[0][3]
     #roi = im[int(y1):int(y2), int(x1):int(x2)]
     x1= int(x1)
     x2= int(x2)
     y1= int(y1)
     y2= int(y2)
     crods.append([x1,y1,x2,y2])

     #print(x1,x2,y1,y2)
     #if(x2<x1): x1, x2 = x2,x1
     #if(y2<y1): y1, y2 = y2,y1
     #print(x)
     #print(numberofBoxes)
     #print(x1,x2,y1,y2)
     cropped = im[y1:y2,x1:x2,::-1]
     #cropped = im[x1:y1,x2:y2]
     plt.axis('off');
     #try:
     sig = im.size/20000
     if sig > 500 and sig < 1000:
       sig = 30
     elif sig > 1000:
       sig = 40
     else:
       sig = 10

     print("size",sig)
     blurred = skimage.filters.gaussian(cropped,sigma=sig)
     plt.imshow(blurred)
     
     plt.savefig(f'cropped/{croppename}-{x}.jpeg', bbox_inches = 'tight',
     pad_inches = 0)
     #except ValueError:  #raised if `y` is empty.
       # pass
     #rawimage.paste(plt.imshow(blurred),(0,0),plt.imshow(blurred))
     #plt.show()
     #im1 = Image.open('testplot.png')
     #rawimage.paste(im1,(x1,y1))
     #os.remove("testplot.png")
     #rawimage.save(f'test/{file_path}', quality=100)

  #value = (value['instances'])


  v = Visualizer(

    im[:, :, ::-1],

    metadata=statement_metadata,

    scale=1.,

    instance_mode=ColorMode.SEGMENTATION

  )


  instances = outputs["instances"].to("cpu")

  instances.remove('pred_masks')

  v = v.draw_instance_predictions(instances)
  plt.axis('off')
  result = v.get_image()[:, :, ::-1]
  i = 0 
  for items in crods:
    
    crim = Image.open(f'cropped/{croppename}-{i}.jpeg')
    crim=crim.resize((items[2]-items[0],items[3]-items[1]))
    i +=1
    rawimage.paste(crim,(items[0],items[1]))
    imgplot = plt.imshow(rawimage)
    plt.axis('off')
    plt.show()
   
  #plt save to file
  write_res = cv2.imwrite(f'annotated_results/{file_name}', result)

annotated_images = [f'annotated_results/{f}' for f in test_df.file_name.unique()]

img = cv2.cvtColor(cv2.imread(annotated_images[0]), cv2.COLOR_BGR2RGB)

plt.imshow(img)

plt.axis('off');

"""ADD KODE FOR LESING AV  CUSTOM BILDER

# StyleGAN
GAN stands for Generative Adverserial Network. The basic idea behind GAN is to make *something* that the human brain can identify as a convincing presentation of that *thing*. 

The goal is to set up a model to go over video files to anonymize human faces working at a pig farm.
"""